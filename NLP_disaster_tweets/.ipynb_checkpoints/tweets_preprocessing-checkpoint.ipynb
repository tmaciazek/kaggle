{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddcdca93",
   "metadata": {},
   "source": [
    "# Natural Language Processing with Disaster Tweets - Data Preprocessing\n",
    "\n",
    "There are two main tasks in this notebook:\n",
    "\n",
    "-  clean the `location` column,\n",
    "-  tokenize the tweets so that they can be further used for word embedding,\n",
    "-  map the words from tokenized tweets to their word embeddings using [glove.twitter.27B](https://nlp.stanford.edu/projects/glove/).\n",
    "\n",
    "We will ignore the `keyword` column, because the keywords usually also appear in the tweet itself. So, we anticipate that the keywords do not give any new context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e7d0790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "import random\n",
    "\n",
    "# text processing libraries\n",
    "import re                                  \n",
    "import string  \n",
    "import nltk                              \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer \n",
    "import contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914d488b",
   "metadata": {},
   "source": [
    "Let's load the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04fa2be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/train.csv')\n",
    "df_test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b4bda46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id location                                               text  target\n",
       "0   1      NaN  Our Deeds are the Reason of this #earthquake M...       1\n",
       "1   4      NaN             Forest fire near La Ronge Sask. Canada       1\n",
       "2   5      NaN  All residents asked to 'shelter in place' are ...       1\n",
       "3   6      NaN  13,000 people receive #wildfires evacuation or...       1\n",
       "4   7      NaN  Just got sent this photo from Ruby #Alaska as ...       1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df_train.drop(columns=['keyword'])\n",
    "df_test = df_test.drop(columns=['keyword'])\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9a98d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location\n",
      "False    7238\n",
      "True     3638\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    pd.isna( pd.concat([df_train['location'], df_test['location']]) ).value_counts()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b95025",
   "metadata": {},
   "source": [
    "## Location column\n",
    "\n",
    "As you can see, there lots of missing entries in the `location` column. Our approach to this problem will be to:\n",
    "-  simplify the data so that the column `location` contains only names of countries,\n",
    "- try to infer the names of the countries from the `text` column (if location is empty), or from the text in the 'location' entry if nonempty,\n",
    "- if the above is not successful, fill in the entry by 'Worldwide'.\n",
    "\n",
    "To this end, we load some geodata from the portal [Geonames](https://public.opendatasoft.com/explore/dataset/geonames-all-cities-with-a-population-1000/table/?disjunctive.cou_name_en&sort=name)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90cf1295",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geo_regions = pd.read_csv('geodata/subcountries.csv', delimiter=';')\n",
    "df_geo_cities = pd.read_csv('geodata/cities.csv', delimiter=';')\n",
    "df_geo_regions = df_geo_regions.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b42e534a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>subcountry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Andorra</td>\n",
       "      <td>Escaldes-Engordany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Andorra</td>\n",
       "      <td>Andorra la Vella</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>Umm al Qaywayn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>Raʼs al Khaymah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>Ash Shāriqah</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                country          subcountry\n",
       "0               Andorra  Escaldes-Engordany\n",
       "1               Andorra    Andorra la Vella\n",
       "2  United Arab Emirates      Umm al Qaywayn\n",
       "3  United Arab Emirates     Raʼs al Khaymah\n",
       "4  United Arab Emirates        Ash Shāriqah"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_geo_regions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0454edf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bridgewater</td>\n",
       "      <td>United States</td>\n",
       "      <td>7841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Brookline</td>\n",
       "      <td>United States</td>\n",
       "      <td>58732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hinsdale</td>\n",
       "      <td>United States</td>\n",
       "      <td>1905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Marshfield</td>\n",
       "      <td>United States</td>\n",
       "      <td>4335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Milton</td>\n",
       "      <td>United States</td>\n",
       "      <td>27003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          city        country  population\n",
       "0  Bridgewater  United States        7841\n",
       "1    Brookline  United States       58732\n",
       "2     Hinsdale  United States        1905\n",
       "3   Marshfield  United States        4335\n",
       "4       Milton  United States       27003"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_geo_cities.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc116312",
   "metadata": {},
   "source": [
    "We remove the cities below 100k population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "233b3f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geo_cities = df_geo_cities.drop(df_geo_cities[df_geo_cities['population']<100000].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22db911d",
   "metadata": {},
   "source": [
    "The matching of cities and political regions to countries will be encoded in two dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c8eb805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# region to country dictionary\n",
    "subcountry_dict = df_geo_regions.set_index('subcountry')['country'].to_dict()\n",
    "\n",
    "#city to country dictionary\n",
    "city_dict = df_geo_cities.set_index('city')['country'].to_dict()\n",
    "\n",
    "#set of all countries\n",
    "countries = set(list(df_geo_regions['country'].values)+list(df_geo_cities['country'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c40ab748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_country(loc_tweet, subcountry_dict, city_dict, countries):\n",
    "    \"\"\"\n",
    "    Attempts to match country to string loc_tweet.\n",
    "    \n",
    "    Checks which of the dict keys (if any) are in the string.\n",
    "    \"\"\"\n",
    "    reg_search = [val for key,val in subcountry_dict.items() if key in loc_tweet]\n",
    "    city_search = [val for key,val in city_dict.items() if key in loc_tweet]\n",
    "    country_search = [cntry for cntry in countries if str(cntry) in loc_tweet]\n",
    "    results = reg_search+city_search+country_search\n",
    "    \n",
    "    if len(results) == 0:\n",
    "        country = 'Worldwide'\n",
    "    elif len(country_search) > 0:\n",
    "        country = random.choice(country_search)\n",
    "    elif 'United States' in results: #if there are a few candidates, pick US\n",
    "        country = 'United States'\n",
    "    elif 'United Kingdom' in results: #if there are a few candidates, pick UK after US\n",
    "        country = 'United Kingdom'\n",
    "    else:\n",
    "        country = random.choice(results) #otherwise, pick randomly\n",
    "    return country"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f249fcec",
   "metadata": {},
   "source": [
    "First, we try to infer the country from the tweet if location is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74173f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_locna_train = df_train['text'].iloc[df_train.index[df_train['location'].isna()]]\n",
    "text_locna_test = df_test['text'].iloc[df_test.index[df_test['location'].isna()]]\n",
    "\n",
    "text_locna_train = text_locna_train.apply(\n",
    "    lambda text: find_country(text, subcountry_dict, city_dict, countries)\n",
    ")\n",
    "\n",
    "text_locna_test = text_locna_test.apply(\n",
    "    lambda text: find_country(text, subcountry_dict, city_dict, countries)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca98a17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the empty location entries by the found ones.\n",
    "df_train['location'] = df_train['location'].fillna(text_locna_train)\n",
    "df_test['location'] = df_test['location'].fillna(text_locna_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6df0bad",
   "metadata": {},
   "source": [
    "Next, try to infer the country from the location. To this end, we normalise locations by replacing a few common abbreviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a0d3a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalised_names = {\n",
    "    'USA' : 'United States',\n",
    "    'US' : 'United States',\n",
    "    'U.S.A.' : 'United States',\n",
    "    'U.S.A' : 'United States',\n",
    "    'U.S.' : 'United States',\n",
    "    'CA' : 'United States',\n",
    "    'UK' : 'United Kingdom',\n",
    "    'NY' : 'United States',\n",
    "    'nyc' : 'United States',\n",
    "    'U.K.' : 'United Kingdom'\n",
    "}\n",
    "\n",
    "def normalise_locations(df, normalised_names): \n",
    "    \"\"\"\n",
    "    Replaces the abbreviations from normalised_names (dict) with their corresponding countries.\n",
    "    \n",
    "    Removes hyphens, commas and slashes.\n",
    "    \"\"\"\n",
    "    df = df.apply(\n",
    "    lambda loc: [\n",
    "        normalised_names[w] if w in normalised_names else w.strip(' ') for w in re.split( r'[,-/]' , str(loc) ) \n",
    "    ]\n",
    "    )\n",
    "    \n",
    "    df = df.apply(\n",
    "    lambda words: ' '.join(words)\n",
    "    )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f3ce649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise the 'location' columns.\n",
    "df_loc_train = normalise_locations(df_train['location'], normalised_names)\n",
    "df_loc_test = normalise_locations(df_test['location'], normalised_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9b5ee9",
   "metadata": {},
   "source": [
    "Next, we infer countries from the normalised `location` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02932fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loc_train = df_loc_train.apply(\n",
    "lambda loc_tweet: find_country(loc_tweet, subcountry_dict, city_dict, countries)\n",
    ")\n",
    "\n",
    "df_loc_test = df_loc_test.apply(\n",
    "lambda loc_tweet: find_country(loc_tweet, subcountry_dict, city_dict, countries)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cbe851",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( pd.concat([df_loc_test,df_loc_train]).value_counts() )\n",
    "print( pd.concat([df_loc_test,df_loc_train]).nunique() )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae08894",
   "metadata": {},
   "source": [
    "The results are OK. We have created extra 1.8K unknown (i.e. 'Worldwide') entries, but if one inspects the original column, one can see that sometimes it is not possible to tell the country from the `location` data.\n",
    "\n",
    "We save the location data in separate files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e67718",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loc_train.to_csv('data/location_train.csv')\n",
    "df_loc_test.to_csv('data/location_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc20c14",
   "metadata": {},
   "source": [
    "# Tweet processing and tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8eccce",
   "metadata": {},
   "source": [
    "First, we extract the tweets from the train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8d1cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_train = df_train['text'].copy()\n",
    "tweets_test = df_test['text'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3249bfc3",
   "metadata": {},
   "source": [
    "Next, we tokenize the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb64d676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(tweet):\n",
    "    \"\"\"\n",
    "    removes url from tweet (string)\n",
    "    \"\"\"\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'', tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f060fb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer(\n",
    "    preserve_case=False, strip_handles=True, reduce_len=True\n",
    ")\n",
    "\n",
    "def clean_tweet(tweet, tokenizer):\n",
    "    \"\"\"\n",
    "    Takes a string (presumably a tweet) and returns a list of tokenized words.\n",
    "    \"\"\"\n",
    "    # Remove hyperlinks\n",
    "    tweet2 = remove_url(tweet)\n",
    "    # Remove hashtags and @\n",
    "    tweet2 = re.sub(r'#', '', tweet2)\n",
    "    tweet2 = re.sub(r'@', '', tweet2)\n",
    "    \n",
    "    # Tokenize the tweet and expand contractions\n",
    "    tweet_tokens = []\n",
    "    for w in tokenizer.tokenize(tweet2):\n",
    "        tweet_tokens = tweet_tokens + tokenizer.tokenize(contractions.fix(w))\n",
    "    \n",
    "    # Remove punctuation\n",
    "    return [w for w in tweet_tokens if w not in string.punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfe4263",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_train = tweets_train.apply(\n",
    "    lambda tweet: clean_tweet(tweet, tokenizer)\n",
    ")\n",
    "\n",
    "tweets_test = tweets_test.apply(\n",
    "    lambda tweet: clean_tweet(tweet, tokenizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162ce939",
   "metadata": {},
   "source": [
    "Next, we move to the task of identifying the word embeddings. We will map each word in the given tokenized tweet to its index in the (**sorted**) GloVe dictionary.\n",
    "\n",
    "We read the GloVe file (**note that this is not uploaded to my GitHub folder, because of the large size of the file**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827a2c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_indices(glove_file):\n",
    "    with open(glove_file, 'r') as f:\n",
    "        words = set()\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "        \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            i = i + 1\n",
    "    return words_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd83261",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_index = read_glove_indices('data/glove.twitter.27B/glove.twitter.27B.25d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aff4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_indinces_train = tweets_train.apply(\n",
    "    lambda tweet: [words_to_index[w] if w in words_to_index else words_to_index['unk'] for w in tweet]\n",
    ")\n",
    "\n",
    "tweets_indinces_test = tweets_test.apply(\n",
    "    lambda tweet: [words_to_index[w] if w in words_to_index else words_to_index['unk'] for w in tweet]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4004584a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweets_indinces_train.head())\n",
    "print(tweets_indinces_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3930e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the indices to files\n",
    "\n",
    "with open(r'data/tweet_indices_train.txt', 'a') as fp:\n",
    "    for ind_list in tweets_indinces_train.values:\n",
    "            ind_arr = np.array(ind_list, dtype=int).T\n",
    "            np.savetxt(fp, ind_arr, fmt='%d', newline = ' ')\n",
    "            fp.write('\\n')\n",
    "    \n",
    "    with open(r'data/tweet_indices_test.txt', 'a') as fp:\n",
    "        for ind_list in tweets_indinces_test.values:\n",
    "            ind_arr = np.array(ind_list, dtype=int).T\n",
    "            np.savetxt(fp, ind_arr, fmt='%d', newline = ' ')\n",
    "            fp.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582e072a",
   "metadata": {},
   "source": [
    "We are now finished with data preprocessing and can move on to actual language processing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9f3061",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
